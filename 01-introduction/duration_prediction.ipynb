{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b944c-c6ff-4348-b0cd-d45b3b041eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c704d-b1bc-4275-b58a-b5dbbaf0322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "file_taxi_january = 'yellow_tripdata_2023-01.parquet'\n",
    "file_taxi_february = 'yellow_tripdata_2023-02.parquet'\n",
    "\n",
    "# 2021 (used in the video examples)\n",
    "file_green_january = 'green_tripdata_2021-01.parquet'\n",
    "file_green_february = 'green_tripdata_2021-02.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28272bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_percentiles_df(df, column='duration_minutes'):\n",
    "    \"\"\" Quickly see the extreme percentiles of a feature \"\"\"    \n",
    "\n",
    "    # upper values\n",
    "    with pd.option_context('display.float_format', '{:.0f}'.format):\n",
    "        print(df[column].describe(percentiles=[0.95, 0.98, 0.99, 0.998]))\n",
    "\n",
    "    # lowest values\n",
    "    with pd.option_context('display.float_format', '{:.0f}'.format):\n",
    "        print(df[column].describe(percentiles=[0.01, 0.05, 0.1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa2f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(filename: str, type) -> pd.DataFrame:\n",
    "    # create a dataframe\n",
    "    if type==\"parquet\":\n",
    "        df = pd.read_parquet(filename, engine='pyarrow')\n",
    "    if type==\"csv\":\n",
    "        df = pd.read_csv(filename)\n",
    "    # take a look\n",
    "    print(f\"Input Dataframe shape: {df.shape}\")\n",
    "    \n",
    "    #df['duration'] = df.tpep_dropoff_datetime - df.tpep_pickup_datetime\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "\n",
    "    # Create a new column that represents the trip duration in minutes instead of data_time objs\n",
    "    df['duration_minutes'] = df.duration.apply(lambda x: x.total_seconds() / 60)\n",
    "    print(f\"Filtered Dataframe shape: {df.shape}\")\n",
    "\n",
    "    # PULOcation = Pickup Location ID, DOLocation = Drop-off location ID\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    # cast to String type\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "        \n",
    "    return df\n",
    "    \n",
    "    # visualize the distribution of trips duration\n",
    "    #sns.histplot(df.duration_minutes)\n",
    "    sns.distplot(df.duration_minutes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb6689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_scale_feature(df, column='duration_minutes'):\n",
    "    \"\"\" Changes a given column into its logarithmic scale\n",
    "     It will create a new column with the prefix 'log_' added to the transformed column name\n",
    "      Returns: a dataframe with the new column added. \"\"\"    \n",
    "\n",
    "    # test logaritmic scale\n",
    "    new_column_name = 'log_' + column\n",
    "\n",
    "    df[new_column_name] = np.log(df[column] + 1)  # Add 1 to avoid log(0) issues\n",
    "    sns.distplot(df[new_column_name])\n",
    "    return df\n",
    "\n",
    "log_scale_feature(df_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2621e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframes for the ML model\n",
    "df_train = read_dataframe(filename=file_green_january, type='parquet')\n",
    "df_val = read_dataframe(filename=file_green_february, type='parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c933464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_percentiles_df(df_train)\n",
    "print(\"- - - - - - - - - -\")\n",
    "check_percentiles_df(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the new distribution\n",
    "print(\"Train dataset\")\n",
    "sns.distplot(df_train.duration_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f0a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Validation dataset\")\n",
    "sns.distplot(df_val.duration_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6787ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DictVectorizer to do one-hot encoding to our categorical variables later\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "numerical = ['trip_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bb0251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the DF into dictionary\n",
    "train_dicts = df2[categorical + numerical] \\\n",
    "    .to_dict(orient='records')\n",
    "\n",
    "# Create data to train the model\n",
    "dv = DictVectorizer()\n",
    "\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "# Preview\n",
    "print(len(dv.feature_names_))\n",
    "dv.feature_names_[:5]\n",
    "\n",
    "# Target variable (prediction)\n",
    "target = 'duration_minutes'\n",
    "# target = 'log_duration_minutes'\n",
    "y_train = df2[target].values\n",
    "\n",
    "# train the model\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = lr.predict(X_train)\n",
    "\n",
    "# Visualize predictions against actual values\n",
    "\n",
    "\n",
    "y_pred_minutes = np.exp(y_pred) - 1  # Convert back to minutes\n",
    "\n",
    "sns.distplot(y_pred, label='Prediction')\n",
    "sns.distplot(y_train, label='Actual data')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46742183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error metrics\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_train, y_pred, squared=False)}')\n",
    "\n",
    "# view how many NA values are in the DF\n",
    "print('Null values in the complete dataframe')\n",
    "df2.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52431125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "# train the model\n",
    "lr = Lasso()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "# predict\n",
    "y_pred = lr.predict(X_train)\n",
    "\n",
    "# Visualize predictions against actual values\n",
    "\n",
    "\n",
    "y_pred_minutes = np.exp(y_pred) - 1  # Convert back to minutes\n",
    "\n",
    "sns.distplot(y_pred, label='Prediction')\n",
    "sns.distplot(y_train, label='Actual data')\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb972ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error metrics\n",
    "mean_squared_error(y_train, y_pred, squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
